#ReinforcementLearning 
A [[Policy Gradient]] algorithm that works in practice.

Here, instead of parameterising the function approximation for a stochastic policy we parameterise a function approximation for a deterministic policy (in the case of continuous action spaces). DPG is expressed in terms of the Expected Gradient of the Q-Value Function and can be estimated much more efficiently than the normal stochastic Policy Gradient. 

DPG provides a [[Compatible Function Approximations]] theorem for DPG to overcome [[Actor-Critic]] Critic approximation bias. 


### Deterministic Policy Gradient Theorem
Given an MDP with action space $\mathbb{R}^k$, with appropriate gradient existence conditions,
$$
\begin{aligned}
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) & =\left.\sum_{s \in \mathcal{N}} \rho^{\pi_D}(s) \cdot \nabla_{\boldsymbol{\theta}} \pi_D(s ; \boldsymbol{\theta}) \cdot \nabla_a Q^{\pi_D}(s, a)\right|_{a=\pi_D(s ; \boldsymbol{\theta})} \\
& =\mathbb{E}_{s \sim \rho^{\pi_D}}\left[\left.\nabla_{\boldsymbol{\theta}} \pi_D(s ; \boldsymbol{\theta}) \cdot \nabla_a Q^{\pi_D}(s, a)\right|_{a=\pi_D(s ; \boldsymbol{\theta})}\right]
\end{aligned}
$$
In practice, we use an Actor-Critic algorithm with a function approximation $Q(s, a ; \boldsymbol{w})$ for the Q-Value Function as the Critic. Since the policy approximated is Deterministic, we need to address the issue of exploration-this is typically done with Off-Policy Control wherein we employ an exploratory (stochastic) behaviour policy, while the policy being approximated (and learnt with DPG) is the target (deterministic) policy. The Expected Return Objective is a bit different in the case of Off-Policy-it is the Expected Q-Value for the target policy under state-occurrence probabilities while following the behaviour policy, and the Off-Policy Deterministic Policy Gradient is an approximate (not exact) formula. We avoid importance sampling in the Actor because DPG doesn't involve an integral over actions, and we avoid importance sampling in the Critic by employing $Q$-Learning. As a result, for Off-Policy Actor-Critic DPG, we update the Critic parameters $w$ and the Actor parameters $\theta$ after each atomic experience in a trace experience generated by the behavior policy.
$$
\begin{gathered}
\Delta \boldsymbol{w}=\alpha_{\boldsymbol{w}} \cdot\left(R_{t+1}+\gamma \cdot Q\left(S_{t+1}, \pi_D\left(S_{t+1} ; \boldsymbol{\theta}\right) ; \boldsymbol{w}\right)-Q\left(S_t, A_t ; \boldsymbol{w}\right)\right) \cdot \nabla_{\boldsymbol{w}} Q\left(S_t, A_t ; \boldsymbol{w}\right) \\
\Delta \boldsymbol{\theta}=\left.\alpha_{\boldsymbol{\theta}} \cdot \nabla_{\boldsymbol{\theta}} \pi_D\left(S_t ; \boldsymbol{\theta}\right) \cdot \nabla_a Q\left(S_t, a ; \boldsymbol{w}\right)\right|_{a=\pi_D\left(S_t ; \boldsymbol{\theta}\right)}
\end{gathered}
$$